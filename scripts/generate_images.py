"""
åœ–ç‰‡ç”Ÿæˆæ¨¡çµ„ - ä½¿ç”¨æœ¬åœ° Stable Diffusion
ç‚ºæ¯å€‹æ®µè½ç”Ÿæˆå°æ‡‰çš„èƒŒæ™¯åœ–ç‰‡
"""

import os
import sys
from typing import List, Dict
from PIL import Image
import torch
from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline
import json
from deep_translator import GoogleTranslator

# è¨­ç½® CUDA è¨˜æ†¶é«”ç®¡ç†ç’°å¢ƒè®Šæ•¸ï¼ˆæ¸›å°‘è¨˜æ†¶é«”ç¢ç‰‡ï¼‰
os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')


class ImageGenerator:
    def __init__(
        self,
        model_type: str = "sdxl",  # "sdxl" or "sd15"
        device: str = None,
        output_dir: str = "images",
        lora_path: str = None,
        lora_scale: float = 0.8,
        checkpoint_path: str = None
    ):
        """
        åˆå§‹åŒ–åœ–ç‰‡ç”Ÿæˆå™¨
        
        Args:
            model_type: æ¨¡å‹é¡å‹ "sdxl" æˆ– "sd15"
            device: è¨­å‚™ ("cuda", "cpu", "mps")
            output_dir: è¼¸å‡ºç›®éŒ„
            lora_path: å¯é¸ LoRA æ¬Šé‡è·¯å¾‘ï¼ˆ.safetensors æˆ–ç›®éŒ„ï¼‰ï¼Œä¹Ÿå¯ç”¨ç’°å¢ƒè®Šæ•¸ LORA_PATH
            lora_scale: LoRA å¼·åº¦ 0~1ï¼Œé è¨­ 0.8ï¼›ä¹Ÿå¯ç”¨ç’°å¢ƒè®Šæ•¸ LORA_SCALE
            checkpoint_path: å¯é¸æœ¬åœ°å®Œæ•´æ¨¡å‹è·¯å¾‘ï¼ˆCivitAI ç­‰ .safetensors/.ckptï¼‰ï¼Œä¹Ÿå¯ç”¨ç’°å¢ƒè®Šæ•¸ CHECKPOINT_PATH
        """
        self.model_type = model_type
        self.output_dir = output_dir
        self.pipeline = None
        self.translator = GoogleTranslator(source='auto', target='en')
        self.base_character_prompt = ""  # ç”¨æ–¼ä¿æŒè§’è‰²ä¸€è‡´æ€§
        self.is_turbo = False  # æ¨™è¨˜æ˜¯å¦ç‚º Turbo æ¨¡å‹
        self.lora_path = lora_path or os.environ.get("LORA_PATH", "").strip() or None
        self.lora_scale = float(os.environ.get("LORA_SCALE", str(lora_scale)))
        self.checkpoint_path = checkpoint_path or os.environ.get("CHECKPOINT_PATH", "").strip() or None
        
        # è¨­å‚™æª¢æ¸¬å’Œè¨ºæ–·
        if device:
            self.device = device
        else:
            if torch.cuda.is_available():
                self.device = "cuda"
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                print(f"âœ… æª¢æ¸¬åˆ° GPU: {gpu_name} ({gpu_memory:.1f} GB VRAM)")
            else:
                self.device = "cpu"
                print("âš ï¸  è­¦å‘Š: æœªæª¢æ¸¬åˆ° CUDAï¼Œå°‡ä½¿ç”¨ CPUï¼ˆé€Ÿåº¦æ¥µæ…¢ï¼‰")
                print("   æç¤º: è«‹å®‰è£ PyTorch CUDA ç‰ˆæœ¬ä»¥ä½¿ç”¨ GPU åŠ é€Ÿ")
                print("   å®‰è£å‘½ä»¤: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"ğŸ–¼ï¸  åœ–ç‰‡ç”Ÿæˆå™¨åˆå§‹åŒ–ï¼Œè¨­å‚™: {self.device}")
    
    def load_model(self):
        """è¼‰å…¥ Stable Diffusion æ¨¡å‹"""
        try:
            print(f"ğŸ“¦ æ­£åœ¨è¼‰å…¥ {self.model_type} æ¨¡å‹...")
            
            # å„ªå…ˆï¼šæœ¬åœ°å®Œæ•´æ¨¡å‹ï¼ˆCivitAI ç­‰ä¸‹è¼‰çš„ .safetensors / .ckptï¼‰
            if self.checkpoint_path and os.path.exists(self.checkpoint_path):
                print(f"ğŸ“‚ å¾æœ¬åœ°æª”æ¡ˆè¼‰å…¥: {self.checkpoint_path}")
                dtype = torch.float16 if self.device == "cuda" else torch.float32
                if self.model_type == "sdxl":
                    self.pipeline = StableDiffusionXLPipeline.from_single_file(
                        self.checkpoint_path,
                        torch_dtype=dtype
                    )
                else:
                    self.pipeline = StableDiffusionPipeline.from_single_file(
                        self.checkpoint_path,
                        torch_dtype=dtype
                    )
                print(f"âœ… å·²è¼‰å…¥æœ¬åœ°æ¨¡å‹ï¼ˆ{self.model_type}ï¼‰")
            elif self.model_type == "sdxl":
                # SDXL æ¨¡å‹ï¼ˆéœ€è¦æ›´å¤š VRAMï¼‰
                model_id = "stabilityai/stable-diffusion-xl-base-1.0"
                self.pipeline = StableDiffusionXLPipeline.from_pretrained(
                    model_id,
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                    use_safetensors=True
                )
            else:
                # SD 1.5 æ¨¡å‹ï¼ˆè¼ƒè¼•é‡ï¼‰
                # ä½¿ç”¨ Turbo æ¨¡å‹ä»¥ç²å¾—æ¥µé€Ÿç”Ÿæˆï¼ˆ1-4 æ­¥å³å¯ï¼‰
                # å„ªå…ˆé †åºï¼šSDXL Turbo (æœ€å¿«) -> SD 1.5 Turbo -> DreamShaper 8 -> Realistic Vision
                models_to_try = [
                    ("stabilityai/sdxl-turbo", "SDXL Turbo - æ¥µé€Ÿæ¨¡å‹ï¼Œ1-4 æ­¥ç”Ÿæˆ", "sdxl"),
                    ("stabilityai/sd-turbo", "SD 1.5 Turbo - æ¥µé€Ÿæ¨¡å‹ï¼Œ1-4 æ­¥ç”Ÿæˆ", "sd15"),
                    ("Lykon/DreamShaper-8", "DreamShaper 8 - æœ€æ–°ç‰ˆæœ¬ï¼Œå¹³è¡¡æ¨¡å‹", "sd15"),
                    ("SG161222/Realistic_Vision_V5.1_noVAE", "Realistic Vision - å¯«å¯¦é¢¨æ ¼", "sd15"),
                    ("runwayml/stable-diffusion-v1-5", "åŸå§‹ SD 1.5 - ç©©å®šç‰ˆæœ¬", "sd15")
                ]
                
                loaded = False
                is_turbo = False
                for model_info in models_to_try:
                    if len(model_info) == 3:
                        model_id, description, model_type = model_info
                    else:
                        model_id, description = model_info
                        model_type = "sd15"
                    
                    try:
                        print(f"ğŸ“¦ å˜—è©¦è¼‰å…¥æ¨¡å‹: {model_id}")
                        print(f"ğŸ’¡ {description}")
                        
                        # æª¢æŸ¥æ˜¯å¦ç‚º Turbo æ¨¡å‹
                        if "turbo" in model_id.lower():
                            is_turbo = True
                            print(f"âš¡ é€™æ˜¯ Turbo æ¨¡å‹ï¼Œå°‡ä½¿ç”¨æ¥µå°‘æ­¥æ•¸ï¼ˆ1-4 æ­¥ï¼‰")
                        
                        # æ ¹æ“šæ¨¡å‹é¡å‹é¸æ“‡ Pipeline
                        if model_type == "sdxl":
                            # å˜—è©¦ safetensors å„ªå…ˆ
                            try:
                                self.pipeline = StableDiffusionXLPipeline.from_pretrained(
                                    model_id,
                                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                                    use_safetensors=True
                                )
                            except:
                                print(f"   âš ï¸  Safetensors è¼‰å…¥å¤±æ•—ï¼Œå˜—è©¦æ¨™æº–æ ¼å¼...")
                                self.pipeline = StableDiffusionXLPipeline.from_pretrained(
                                    model_id,
                                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                                    use_safetensors=False
                                )
                        else:
                            # SD 1.5 æ¨¡å‹
                            try:
                                self.pipeline = StableDiffusionPipeline.from_pretrained(
                                    model_id,
                                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                                    use_safetensors=True
                                )
                            except:
                                print(f"   âš ï¸  Safetensors è¼‰å…¥å¤±æ•—ï¼Œå˜—è©¦æ¨™æº–æ ¼å¼...")
                                self.pipeline = StableDiffusionPipeline.from_pretrained(
                                    model_id,
                                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                                    use_safetensors=False
                                )
                        
                        # ä¿å­˜æ˜¯å¦ç‚º Turbo æ¨¡å‹
                        self.is_turbo = is_turbo
                        print(f"âœ… æˆåŠŸè¼‰å…¥: {model_id}")
                        loaded = True
                        break
                    except Exception as e:
                        print(f"   âŒ {model_id} è¼‰å…¥å¤±æ•—: {str(e)[:100]}...")
                        continue
                
                if not loaded:
                    raise Exception("æ‰€æœ‰æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç¶²çµ¡é€£æ¥æˆ–æ¨¡å‹å¯ç”¨æ€§")
            
            # å„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨ï¼ˆé‡å° RTX 3070 ç­‰ 8GB VRAM GPUï¼‰
            # é‡è¦ï¼šä¸ä½¿ç”¨ sequential CPU offloadï¼ˆå¤ªæ…¢ï¼‰ï¼Œä½¿ç”¨æ¨™æº– GPU æ¨¡å¼
            if self.device == "cuda":
                # ç›´æ¥è¼‰å…¥åˆ° GPUï¼ˆæœ€å¿«ï¼‰
                self.pipeline = self.pipeline.to(self.device)
                # è¼•é‡å„ªåŒ–ï¼ˆä¸å½±éŸ¿é€Ÿåº¦å¤ªå¤šï¼‰
                self.pipeline.enable_attention_slicing(4)  # åˆ‡ç‰‡å¤§å° 4ï¼ˆè¼ƒå¿«ï¼‰
                # ä½¿ç”¨ VAE tilingï¼ˆç¯€çœ VRAMï¼Œä¸å½±éŸ¿é€Ÿåº¦ï¼‰
                if hasattr(self.pipeline, 'vae'):
                    if hasattr(self.pipeline.vae, 'enable_tiling'):
                        self.pipeline.vae.enable_tiling()
                print("ğŸ’¾ å·²å•Ÿç”¨è¼•é‡å„ªåŒ–ï¼ˆGPU æ¨¡å¼ï¼Œé€Ÿåº¦å„ªå…ˆï¼‰")
            else:
                self.pipeline = self.pipeline.to(self.device)
            
            # å¯é¸ï¼šè¼‰å…¥ LoRA æ¬Šé‡ï¼ˆè¦‹ FINE_TUNING_GUIDE.mdï¼‰
            if self.lora_path and os.path.exists(self.lora_path):
                try:
                    if os.path.isfile(self.lora_path):
                        lora_dir = os.path.dirname(self.lora_path)
                        weight_name = os.path.basename(self.lora_path)
                        self.pipeline.load_lora_weights(
                            lora_dir,
                            weight_name=weight_name,
                            adapter_name="story_style"
                        )
                    else:
                        self.pipeline.load_lora_weights(
                            self.lora_path,
                            adapter_name="story_style"
                        )
                    self.pipeline.set_adapters(["story_style"], adapter_weights=[self.lora_scale])
                    print(f"âœ… LoRA å·²è¼‰å…¥: {self.lora_path} (scale={self.lora_scale})")
                except Exception as lora_err:
                    print(f"âš ï¸  LoRA è¼‰å…¥å¤±æ•—ï¼ˆå°‡ä¸ä½¿ç”¨ LoRAï¼‰: {lora_err}")
            elif self.lora_path:
                print(f"âš ï¸  LoRA è·¯å¾‘ä¸å­˜åœ¨ï¼Œè·³é: {self.lora_path}")
            
            print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise
    
    def translate_to_english(self, text: str) -> str:
        """å°‡ä¸­æ–‡ç¿»è­¯ç‚ºè‹±æ–‡ï¼ˆSD å°è‹±æ–‡ç†è§£æ›´å¥½ï¼‰"""
        try:
            # æª¢æ¸¬æ˜¯å¦åŒ…å«ä¸­æ–‡
            if any('\u4e00' <= char <= '\u9fff' for char in text):
                translated = self.translator.translate(text)
                print(f"ğŸ”¤ ç¿»è­¯: {text[:20]}... -> {translated[:50]}...")
                return translated
            return text
        except Exception as e:
            print(f"âš ï¸  ç¿»è­¯å¤±æ•—: {e}")
            return text

    def _get_emotion_prompt_from_llm(self, emotion: str) -> str:
        """
        æ ¹æ“š LLM åˆ†æçš„æƒ…æ„Ÿè¿”å›ç›¸æ‡‰çš„æç¤ºè©
        
        Args:
            emotion: LLM åˆ†æçš„æƒ…æ„Ÿï¼ˆpositive/negative/neutralï¼‰
            
        Returns:
            æƒ…æ„Ÿç›¸é—œçš„æç¤ºè©ç‰‡æ®µ
        """
        emotion_lower = emotion.lower() if emotion else ""
        
        if "positive" in emotion_lower or "happy" in emotion_lower or "joy" in emotion_lower:
            return "joyful atmosphere, warm mood, uplifting emotions, bright lighting, positive energy"
        elif "negative" in emotion_lower or "sad" in emotion_lower or "sorrow" in emotion_lower:
            return "melancholic atmosphere, somber mood, emotional depth, dramatic lighting, expressive emotions"
        elif "neutral" in emotion_lower or "calm" in emotion_lower:
            return "calm atmosphere, peaceful mood, serene emotions, soft lighting, contemplative"
        else:
            # é»˜èªè¿”å›ç©ºï¼Œè®“ä¸‹é¢çš„å‡½æ•¸è™•ç†
            return ""
    
    def _analyze_emotional_context(self, scene_description: str, story_text: str = None) -> str:
        """
        åˆ†æå ´æ™¯çš„æƒ…æ„Ÿè‰²å½©ï¼Œæ·»åŠ ç›¸æ‡‰çš„æƒ…æ„Ÿè©å½™åˆ°æç¤ºè©
        
        Args:
            scene_description: å ´æ™¯æè¿°
            story_text: æ•…äº‹æ–‡æœ¬
            
        Returns:
            æƒ…æ„Ÿç›¸é—œçš„æç¤ºè©ç‰‡æ®µ
        """
        # åˆä½µæ–‡æœ¬é€²è¡Œåˆ†æ
        combined_text = (scene_description + " " + (story_text or "")).lower()
        
        # è² é¢æƒ…æ„Ÿé—œéµè©
        negative_keywords = [
            "æ‚²å‚·", "é›£é", "ç—›è‹¦", "çµ•æœ›", "å¤±æœ›", "æ²®å–ª", "æ†‚æ„", "å“€å‚·",
            "å®³æ€•", "ææ‡¼", "é©šæ", "æ“”æ†‚", "ç„¦æ…®", "ä¸å®‰",
            "ç”Ÿæ°£", "æ†¤æ€’", "æƒ±æ€’", "ä¸æ»¿", "æ€¨æ¨",
            "å¤±æ•—", "å¤±å»", "æ­»äº¡", "ç½é›£", "ä¸å¹¸", "å›°é›£", "å±éšª",
            "sad", "sorrow", "pain", "despair", "disappointment", "depression",
            "fear", "afraid", "worried", "anxious", "scared",
            "angry", "rage", "furious", "upset", "hate",
            "failure", "loss", "death", "disaster", "misfortune", "difficulty", "danger"
        ]
        
        # æ­£é¢æƒ…æ„Ÿé—œéµè©
        positive_keywords = [
            "å¿«æ¨‚", "é–‹å¿ƒ", "é«˜èˆˆ", "å–œæ‚…", "èˆˆå¥®", "æ­¡æ¨‚", "æ„‰å¿«", "æ»¿è¶³",
            "æˆåŠŸ", "å‹åˆ©", "ç²å¾—", "æˆå°±", "å¸Œæœ›", "ç¾å¥½", "å¹¸ç¦", "å’Œå¹³",
            "æ…¶ç¥", "æ­¡å‘¼", "è®šç¾", "æ„Ÿè¬", "æ„›", "å‹èª¼",
            "happy", "joy", "pleasure", "excited", "delighted", "cheerful", "content",
            "success", "victory", "achieve", "accomplishment", "hope", "beautiful", "peace",
            "celebrate", "cheer", "praise", "thank", "love", "friendship"
        ]
        
        # ä¸­æ€§/å¹³éœæƒ…æ„Ÿé—œéµè©
        neutral_keywords = [
            "å¹³éœ", "å®‰å¯§", "æ²‰æ€", "æ€è€ƒ", "å°ˆæ³¨", "èªçœŸ",
            "calm", "peaceful", "serene", "contemplative", "thoughtful", "focused"
        ]
        
        # æª¢æ¸¬æƒ…æ„Ÿ
        negative_count = sum(1 for keyword in negative_keywords if keyword in combined_text)
        positive_count = sum(1 for keyword in positive_keywords if keyword in combined_text)
        neutral_count = sum(1 for keyword in neutral_keywords if keyword in combined_text)
        
        # æ ¹æ“šæƒ…æ„Ÿæ·»åŠ ç›¸æ‡‰çš„æç¤ºè©
        if negative_count > positive_count and negative_count > 0:
            # è² é¢æƒ…æ„Ÿå ´æ™¯
            return "melancholic atmosphere, somber mood, emotional depth, dramatic lighting, expressive emotions"
        elif positive_count > negative_count and positive_count > 0:
            # æ­£é¢æƒ…æ„Ÿå ´æ™¯
            return "joyful atmosphere, warm mood, uplifting emotions, bright lighting, positive energy"
        elif neutral_count > 0:
            # ä¸­æ€§/å¹³éœå ´æ™¯
            return "calm atmosphere, peaceful mood, serene emotions, soft lighting, contemplative"
        else:
            # é»˜èªï¼šæ ¹æ“šå ´æ™¯æè¿°æ¨æ–·
            if any(word in combined_text for word in ["æ‚²", "å‚·", "å¤±", "æ•—", "æ­»", "sad", "loss", "fail"]):
                return "emotional depth, expressive mood"
            elif any(word in combined_text for word in ["å–œ", "æ¨‚", "æˆ", "å‹", "æ­¡", "happy", "success", "win"]):
                return "joyful mood, positive atmosphere"
            else:
                return ""  # ç„¡æ˜é¡¯æƒ…æ„Ÿï¼Œä¸æ·»åŠ 
    
    def check_safety(self, prompt: str) -> bool:
        """ç°¡å–®çš„å®‰å…¨æª¢æŸ¥"""
        unsafe_words = ["nsfw", "nude", "sex", "naked", "porn", "explicit", "gore", "blood", "violence"]
        return not any(word in prompt.lower() for word in unsafe_words)

    def _build_character_prompt(self, character: Dict) -> str:
        """
        å¾è§’è‰²è³‡è¨Šçµ„å‡ºç²¾æº–çš„è‹±æ–‡æç¤ºç‰‡æ®µï¼šç‰©ç¨®ã€æ€§åˆ¥ã€å¹´é½¡ã€æœè£ã€æ°‘æ—ã€‚
        """
        if not character or not isinstance(character, dict):
            return ""
        parts = []
        breed = (character.get("breed") or "").strip()
        if breed:
            parts.append(breed)
        gender = (character.get("gender") or "").strip().lower()
        if gender in ("male", "female"):
            parts.append(gender)
        age = (character.get("age") or "").strip().lower()
        if age in ("child", "young", "adult", "elder"):
            parts.append(age)
        clothes = (character.get("clothes") or "").strip()
        if clothes:
            parts.append(clothes)
        nation = (character.get("nation") or "").strip()
        if nation:
            parts.append(f"{nation} style")
        if not parts:
            return ""
        raw = ", ".join(parts)
        return self.translate_to_english(raw) if any("\u4e00" <= c <= "\u9fff" for c in raw) else raw

    def generate_image(
        self,
        scene_description: str,
        style: str = "cinematic",
        output_path: str = None,
        width: int = 640,  # å¹³è¡¡è§£æåº¦ï¼ˆå¤ªé«˜æœƒå°è‡´å½å½±å’Œé‡è¤‡éƒ¨åˆ†ï¼‰
        height: int = 1152,  # ä¿æŒ 9:16 æ¯”ä¾‹
        story_title: str = None,
        story_text: str = None,
        paragraph_emotion: str = None,  # LLM åˆ†æçš„æƒ…æ„Ÿ
        character: Dict = None,  # main_character: breed, gender, age, clothes, nation
        action: str = None,  # æ­¤æ®µä¸­äººç‰©æ­£åœ¨åšä»€éº¼
        image_prompt: str = None  # LLM è¼¸å‡ºçš„é—œéµå­—ä¸²ï¼ˆé€—è™Ÿåˆ†éš”ï¼Œtag é¢¨æ ¼ï¼‰ï¼Œå„ªå…ˆä½¿ç”¨
    ) -> str:
        """
        ç”Ÿæˆå–®å¼µåœ–ç‰‡
        
        Args:
            scene_description: å ´æ™¯æè¿°ï¼ˆç’°å¢ƒã€è¦–è¦ºç´°ç¯€ï¼‰
            style: é¢¨æ ¼é¸é …
            output_path: è¼¸å‡ºè·¯å¾‘
            image_prompt: è‹¥æä¾›å‰‡ä½œç‚ºä¸»é«” positive promptï¼ˆé—œéµå­—ä¸²ï¼Œé€—è™Ÿåˆ†éš”ï¼‰ï¼Œå–ä»£å¾ scene/character çµ„æˆçš„å¥å­
        """
        if self.pipeline is None:
            self.load_model()
            
        # LLM æä¾›çš„é—œéµå­—ä¸²ï¼ˆtag é¢¨æ ¼ï¼‰å„ªå…ˆä½œç‚ºä¸» prompt
        # æ³¨æ„ï¼šé—œéµå­—é †åºå¾ˆé‡è¦ï¼Œå‰é¢çš„é—œéµå­—æ¬Šé‡æ›´é«˜
        keyword_prompt = (image_prompt or "").strip()
        if keyword_prompt and any(c in keyword_prompt for c in "abcdefghijklmnopqrstuvwxyz"):
            # è‹¥æœ‰ä¸­æ–‡å‰‡ç¿»è­¯æˆè‹±æ–‡
            if any("\u4e00" <= c <= "\u9fff" for c in keyword_prompt):
                keyword_prompt = self.translate_to_english(keyword_prompt)
            # ä¿ç•™åŸå§‹é †åºï¼ˆå‰é¢çš„é—œéµå­—æ¬Šé‡æ›´é«˜ï¼‰ï¼Œåªæ¸…ç†ç©ºç™½
            keyword_prompt = ", ".join(t.strip() for t in keyword_prompt.split(",") if t.strip())
            
        # ç¿»è­¯å ´æ™¯æè¿°ï¼ˆç„¡ keyword_prompt æ™‚æˆ–ä½œç‚º fallback ç”¨ï¼‰
        english_description = self.translate_to_english(scene_description)
        
        # åˆ†æå ´æ™¯çš„æƒ…æ„Ÿè‰²å½©
        # å„ªå…ˆä½¿ç”¨ LLM åˆ†æçš„æƒ…æ„Ÿï¼Œå¦‚æœæ²’æœ‰å‰‡å¾æ–‡æœ¬åˆ†æ
        if paragraph_emotion:
            # ä½¿ç”¨ LLM åˆ†æçš„æƒ…æ„Ÿ
            emotional_context = self._get_emotion_prompt_from_llm(paragraph_emotion)
            print(f"ğŸ’­ ä½¿ç”¨ LLM åˆ†æçš„æƒ…æ„Ÿ: {paragraph_emotion}")
        else:
            # å¾æ–‡æœ¬åˆ†ææƒ…æ„Ÿ
            emotional_context = self._analyze_emotional_context(scene_description, story_text)
            print(f"ğŸ’­ å¾æ–‡æœ¬åˆ†æçš„æƒ…æ„Ÿ: {emotional_context[:50] if emotional_context else 'ä¸­æ€§'}")
        
        # å®šç¾©é¢¨æ ¼æç¤ºè©ï¼ˆéå¯«å¯¦é¢¨æ ¼å„ªå…ˆï¼ŒåŠ å¼·ä¸­åœ‹é¢¨æ ¼ï¼‰
        style_prompts = {
            "anime": "anime style, japanese anime studio style, cel shaded, high quality anime art, vibrant colors, stylized",
            "chinese_ink": "Chinese ink painting style, traditional Chinese shuimo painting, watercolor, monochrome with subtle color accents, artistic brushwork, traditional Chinese art, elegant and refined",
            "cinematic": "cinematic lighting, movie scene, photorealistic, 8k, dramatic atmosphere, depth of field",
            "ancient": "ancient Chinese illustration style, traditional Chinese painting, classical Chinese art, traditional art, stylized, non-photorealistic, historical Chinese aesthetics",
            "fantasy": "fantasy art style, magical atmosphere, vibrant colors, ethereal, stylized illustration",
            "horror": "dark art style, eerie atmosphere, stylized illustration, non-photorealistic",
            "hand_drawn": "hand-drawn illustration, sketch style, artistic drawing, detailed linework, stylized"
        }
        chosen_style = style_prompts.get(style, style_prompts["anime"])  # é»˜èª anime
        
        # å¦‚æœæ˜¯ä¸­åœ‹é¢¨æ ¼ï¼Œæ·»åŠ æ›´å¤šä¸­åœ‹å…ƒç´ æç¤º
        if style in ["chinese_ink", "ancient"]:
            print(f"ğŸ‡¨ğŸ‡³ ä½¿ç”¨ä¸­åœ‹å‚³çµ±é¢¨æ ¼: {style}")
        
        # é¢¨æ ¼èˆ‡å“è³ªå°¾ç¶´ï¼ˆå…©ç¨®è·¯å¾‘å…±ç”¨ï¼‰
        style_suffix = f"{chosen_style}, detailed, stylized, clear composition, vertical format, simple background, dynamic, highly detailed, sharp focus, high resolution"
        
        if keyword_prompt:
            # ä½¿ç”¨ LLM è¼¸å‡ºçš„é—œéµå­—ä¸²ä½œç‚ºä¸»é«” promptï¼ˆtag é¢¨æ ¼ï¼Œå¤šé—œéµå­—ï¼‰
            # é—œéµå­—é †åºå·²ç”± LLM æ±ºå®šï¼ˆå‰é¢çš„æ¬Šé‡æ›´é«˜ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
            # ä¸å¼·åˆ¶æ·»åŠ è§’è‰²ï¼ˆLLM å·²æ±ºå®šæ˜¯å¦åŒ…å«è§’è‰²ï¼‰
            prompt_parts = ["masterpiece, best quality", keyword_prompt]
            # åªåœ¨å¿…è¦æ™‚æ·»åŠ æƒ…æ„Ÿä¸Šä¸‹æ–‡ï¼ˆå¦‚æœ LLM çš„ prompt ä¸­æ²’æœ‰æ˜é¡¯æƒ…æ„Ÿæ¨™è¨˜ï¼‰
            if emotional_context and not any(emotion_word in keyword_prompt.lower() for emotion_word in ["joyful", "melancholic", "calm", "happy", "sad", "focused", "determined", "eager"]):
                # å¦‚æœ LLM prompt ä¸­æ²’æœ‰æƒ…æ„Ÿç›¸é—œé—œéµå­—ï¼Œæ‰æ·»åŠ 
                prompt_parts.append(emotional_context)
            prompt_parts.append(style_suffix)
            prompt = ", ".join(prompt_parts)
            tag_count = len([t.strip() for t in keyword_prompt.split(",") if t.strip()])
            print(f"ğŸ“ ä½¿ç”¨ LLM é—œéµå­— promptï¼ˆ{tag_count} tagsï¼Œé †åºå·²ä¿ç•™ï¼‰")
        else:
            # Fallbackï¼šå¾è§’è‰²ã€æƒ…æ„Ÿã€å‹•ä½œã€å ´æ™¯çµ„å¥ï¼ˆç•¶ LLM æ²’æœ‰æä¾› image_prompt æ™‚ï¼‰
            # æ³¨æ„ï¼šè§’è‰²ä¸ä¸€å®šéœ€è¦å­˜åœ¨ï¼Œæ ¹æ“šå ´æ™¯æè¿°åˆ¤æ–·
            character_subject = ""
            # åªåœ¨å ´æ™¯æè¿°æˆ–æ–‡æœ¬ä¸­æ˜ç¢ºæåˆ°è§’è‰²æ™‚æ‰æ·»åŠ 
            if character:
                # æª¢æŸ¥å ´æ™¯æè¿°æˆ–æ–‡æœ¬ä¸­æ˜¯å¦æåˆ°è§’è‰²ç›¸é—œå…§å®¹
                scene_lower = (scene_description + " " + (story_text or "")).lower()
                has_character_mention = any(
                    word in scene_lower for word in 
                    ["äºº", "è§’è‰²", "ä¸»è§’", "ä»–", "å¥¹", "person", "character", "man", "woman", "boy", "girl", "people"]
                )
                if has_character_mention:
                    character_subject = self._build_character_prompt(character)
                    if character_subject and not self.base_character_prompt and story_title:
                        self.base_character_prompt = character_subject
                    if not self.base_character_prompt and story_title:
                        self.base_character_prompt = "consistent character"
            
            action_english = ""
            if action and str(action).strip():
                action_english = self.translate_to_english(str(action).strip())
            
            # æŒ‰é‡è¦æ€§æ’åºï¼šå“è³ªæ¨™ç±¤ â†’ è§’è‰²ï¼ˆå¦‚æœå­˜åœ¨ï¼‰â†’ æƒ…æ„Ÿ â†’ å‹•ä½œ â†’ å ´æ™¯ â†’ é¢¨æ ¼
            prompt_parts = ["masterpiece, best quality"]
            if character_subject:
                prompt_parts.append(f"({character_subject}:1.2)")
            elif self.base_character_prompt:
                prompt_parts.append(f"({self.base_character_prompt}:1.2)")
            if emotional_context:
                prompt_parts.append(emotional_context)
            if action_english:
                prompt_parts.append(action_english)
            prompt_parts.append(english_description)
            prompt_parts.append(style_suffix)
            prompt = ", ".join(prompt_parts)
            
            words = prompt.split()
            if len(words) > 75:
                essential_parts = ["masterpiece, best quality"]
                if character_subject:
                    essential_parts.append(f"({character_subject}:1.2)")
                elif self.base_character_prompt:
                    essential_parts.append(f"({self.base_character_prompt}:1.2)")
                if emotional_context:
                    essential_parts.append(emotional_context)
                if action_english:
                    essential_parts.append(action_english)
                essential_parts.append(english_description[:100] if len(english_description) > 100 else english_description)
                essential_parts.append(f"{chosen_style}, vertical format")
                prompt = ", ".join(essential_parts)
        
        print(f"ğŸ“ æç¤ºè©é•·åº¦: {len(prompt.split())} è©ï¼ˆç´„ {len(prompt.split()) * 1.3:.0f} tokensï¼‰")
        
        # Negative Prompt: å¼·åŒ–ä»¥æ¸›å°‘è§£å‰–éŒ¯èª¤å’Œé‡è¤‡éƒ¨åˆ†
        negative_prompt = "nsfw, nude, explicit, sexual, modern clothing, modern architecture, unrelated objects, blurry, low quality, distorted, watermark, bad anatomy, extra limbs, extra fingers, extra arms, extra legs, duplicated body parts, malformed limbs, missing limbs, fused fingers, too many fingers, cropped, out of frame, jpeg artifacts, text, signature, deformed, disfigured, mutation, mutated, ugly, bad proportions, extra digits, fewer digits, missing digits, bad hands, bad feet"
        
        # å®‰å…¨æª¢æŸ¥
        if not self.check_safety(prompt):
            print("âš ï¸  æç¤ºè©åŒ…å«ä¸å®‰å…¨å…§å®¹ï¼Œå·²è·³é")
            return None

        try:
            preview = (keyword_prompt[:60] + "...") if keyword_prompt else (english_description[:50] + "...")
            print(f"ğŸ¨ æ­£åœ¨ç”Ÿæˆåœ–ç‰‡: {preview}")
            
            # æ¸…é™¤ CUDA å¿«å–
            if self.device == "cuda":
                torch.cuda.empty_cache()
            
            # è¨­ç½®ç”Ÿæˆå™¨
            generator = None
            if self.device == "cuda":
                generator = torch.Generator(device="cuda")
                generator.manual_seed(42)  # å›ºå®šç¨®å­ä»¥æé«˜ä¸€è‡´æ€§
            
            # ç”Ÿæˆåœ–ç‰‡ (é«˜è³ªé‡å„ªåŒ–ï¼Œå„ªå…ˆç´°ç¯€)
            # ä½¿ç”¨æ›´å¤šæ­¥æ•¸å’Œæ›´å¥½çš„èª¿åº¦å™¨ä»¥ç²å¾—æ›´å¥½ç´°ç¯€
            if self.is_turbo:
                # Turbo æ¨¡å‹ï¼šå¯ä»¥ç”¨æ›´å¤šæ­¥æ•¸ç²å¾—æ›´å¥½è³ªé‡
                num_steps = 4  # Turbo æ¨¡å‹ç”¨ 4 æ­¥ï¼ˆæ¯” 1 æ­¥è³ªé‡æ›´å¥½ï¼‰
                guidance_scale = 1.0  # Turbo æ¨¡å‹å¯ä»¥ç”¨å°‘é‡ guidance
                print("âš¡ ä½¿ç”¨ Turbo æ¨¡å‹ï¼ˆ4 æ­¥ç”Ÿæˆï¼Œå¹³è¡¡é€Ÿåº¦èˆ‡è³ªé‡ï¼‰")
            else:
                # æ™®é€šæ¨¡å‹ï¼šä½¿ç”¨é«˜è³ªé‡èª¿åº¦å™¨ï¼Œæ›´å¤šæ­¥æ•¸
                try:
                    from diffusers import DPMSolverMultistepScheduler
                    # ä½¿ç”¨ DPM++ 2M Karrasï¼ˆé«˜è³ªé‡ï¼‰
                    self.pipeline.scheduler = DPMSolverMultistepScheduler.from_config(
                        self.pipeline.scheduler.config,
                        use_karras_sigmas=True
                    )
                    num_steps = 20  # 20 æ­¥æ˜¯è³ªé‡èˆ‡é€Ÿåº¦çš„æœ€ä½³å¹³è¡¡ï¼ˆå¤ªå¤šæ­¥æœƒç”¢ç”Ÿå½å½±ï¼‰
                    print("ğŸ¨ ä½¿ç”¨é«˜è³ªé‡èª¿åº¦å™¨ (DPM++ 2M Karras, 20 steps)")
                except:
                    try:
                        from diffusers import EulerAncestralDiscreteScheduler
                        self.pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(
                            self.pipeline.scheduler.config
                        )
                        num_steps = 20
                        print("ğŸ¨ ä½¿ç”¨é«˜è³ªé‡èª¿åº¦å™¨ (Euler Ancestral, 20 steps)")
                    except:
                        num_steps = 20
                        print("âš™ï¸  ä½¿ç”¨é»˜èªèª¿åº¦å™¨ (20 steps)")
                guidance_scale = 7.0  # 7.0 æ˜¯æ¨™æº–å€¼ï¼Œå¤ªé«˜æœƒéåº¦é£½å’Œ
            
            print(f"â³ é–‹å§‹ç”Ÿæˆï¼ˆé è¨ˆéœ€è¦ 30-50 ç§’ï¼‰...")
            import time
            start_time = time.time()
            
            image = self.pipeline(
                prompt=prompt,
                negative_prompt=negative_prompt if not self.is_turbo else None,  # Turbo æ¨¡å‹ä¸éœ€è¦ negative prompt
                width=width,
                height=height,
                num_inference_steps=num_steps,
                guidance_scale=guidance_scale,
                generator=generator,
            ).images[0]
            
            elapsed = time.time() - start_time
            print(f"âœ… ç”Ÿæˆå®Œæˆï¼Œè€—æ™‚: {elapsed:.1f} ç§’")
            
            # ç”Ÿæˆå¾Œæ¸…é™¤å¿«å–
            if self.device == "cuda":
                torch.cuda.empty_cache()
            
            # ä¿å­˜åœ–ç‰‡
            if output_path is None:
                import time
                timestamp = int(time.time())
                output_path = os.path.join(self.output_dir, f"image_{timestamp}.png")
            
            image.save(output_path)
            print(f"âœ… åœ–ç‰‡å·²ä¿å­˜: {output_path}")
            
            return output_path
            
        except Exception as e:
            print(f"âŒ åœ–ç‰‡ç”Ÿæˆå¤±æ•—: {e}")
            # å¦‚æœå¤±æ•—ï¼Œå˜—è©¦é™ä½è§£æåº¦é‡è©¦
            if "out of memory" in str(e).lower() and width > 512:
                print("âš ï¸  VRAM ä¸è¶³ï¼Œå˜—è©¦é™ä½è§£æåº¦é‡è©¦...")
                return self.generate_image(
                    scene_description, style, output_path,
                    width=512, height=896,
                    story_title=story_title, story_text=story_text,
                    paragraph_emotion=paragraph_emotion, character=character, action=action,
                    image_prompt=image_prompt,
                )
            raise
    
    def generate_images_for_script(self, script_data: Dict, style: str = "cinematic") -> List[str]:
        """
        ç‚ºæ•´å€‹åŠ‡æœ¬ç”Ÿæˆæ‰€æœ‰åœ–ç‰‡
        
        Args:
            script_data: åŠ‡æœ¬æ•¸æ“šï¼ˆåŒ…å« paragraphs å’Œ titleï¼‰
            style: åœ–ç‰‡é¢¨æ ¼
            
        Returns:
            åœ–ç‰‡è·¯å¾‘åˆ—è¡¨
        """
        paragraphs = script_data.get("paragraphs", [])
        story_title = script_data.get("title", "")
        image_paths = []
        
        print(f"ğŸ–¼ï¸  é–‹å§‹ç‚º {len(paragraphs)} å€‹æ®µè½ç”Ÿæˆåœ–ç‰‡...")
        print(f"ğŸ“– æ•…äº‹æ¨™é¡Œ: {story_title}")
        
        # é‡ç½®è§’è‰²ç‰¹å¾µ
        self.base_character_prompt = ""
        
        # ç²å–æ•´é«”æƒ…æ„Ÿèˆ‡ä¸»è§’è³‡è¨Šï¼ˆè‹¥ LLM æœ‰æä¾›ï¼‰
        overall_emotion = script_data.get("emotion", None)
        main_character = script_data.get("main_character", None)
        # è‹¥è…³æœ¬æœªæä¾› main_characterï¼Œä»å¯åªç”¨ scene / emotion / action
        style_override = script_data.get("style", style)
        
        for i, paragraph in enumerate(paragraphs):
            scene = paragraph.get("scene", paragraph.get("text", ""))
            text = paragraph.get("text", "")
            paragraph_emotion = paragraph.get("emotion", overall_emotion)
            action = paragraph.get("action", "").strip() or None
            image_prompt = paragraph.get("image_prompt", "").strip() or None  # LLM è¼¸å‡ºçš„é—œéµå­—ä¸²ï¼ˆtag é¢¨æ ¼ï¼‰
            output_path = os.path.join(self.output_dir, f"scene_{i+1:02d}.png")
            
            try:
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                img_path = self.generate_image(
                    scene_description=scene,
                    style=style_override or style,
                    output_path=output_path,
                    story_title=story_title,
                    story_text=text,
                    paragraph_emotion=paragraph_emotion,
                    character=main_character,
                    action=action,
                    image_prompt=image_prompt,
                )
                image_paths.append(img_path)
                
                # ç”Ÿæˆå¾Œæ¸…é™¤å¿«å–
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
            except torch.cuda.OutOfMemoryError as e:
                print(f"âš ï¸  æ®µè½ {i+1} åœ–ç‰‡ç”Ÿæˆå¤±æ•—: VRAM ä¸è¶³")
                print(f"   å˜—è©¦æ¸…ç†è¨˜æ†¶é«”ä¸¦é‡è©¦...")
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.ipc_collect()
                # è·³éé€™å¼µåœ–ç‰‡
                continue
            except Exception as e:
                print(f"âš ï¸  æ®µè½ {i+1} åœ–ç‰‡ç”Ÿæˆå¤±æ•—: {e}")
                # æ¸…é™¤å¿«å–å¾Œç¹¼çºŒ
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                continue
        
        print(f"âœ… å…±ç”Ÿæˆ {len(image_paths)} å¼µåœ–ç‰‡")
        return image_paths


def main():
    """æ¸¬è©¦ç”¨ä¸»å‡½æ•¸"""
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python generate_images.py <script.json> [style]")
        sys.exit(1)
    
    script_file = sys.argv[1]
    style = sys.argv[2] if len(sys.argv) > 2 else "cinematic"
    
    with open(script_file, "r", encoding="utf-8") as f:
        script_data = json.load(f)
    
    generator = ImageGenerator()
    
    try:
        image_paths = generator.generate_images_for_script(script_data, style)
        print(f"\nç”Ÿæˆçš„åœ–ç‰‡: {image_paths}")
    except Exception as e:
        print(f"éŒ¯èª¤: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()





